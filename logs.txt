* 
* ==> Audit <==
* |------------|---------------------|----------|--------|---------|---------------------|---------------------|
|  Command   |        Args         | Profile  |  User  | Version |     Start Time      |      End Time       |
|------------|---------------------|----------|--------|---------|---------------------|---------------------|
| start      | --driver=virtualbox | minikube | akshay | v1.31.2 | 17 Oct 23 09:39 IST |                     |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 09:39 IST | 17 Oct 23 09:44 IST |
| start      | --driver=docker     | minikube | akshay | v1.31.2 | 17 Oct 23 09:56 IST | 17 Oct 23 09:56 IST |
| docker-env |                     | minikube | akshay | v1.31.2 | 17 Oct 23 10:02 IST | 17 Oct 23 10:02 IST |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 10:18 IST | 17 Oct 23 10:18 IST |
| service    | frontend            | minikube | akshay | v1.31.2 | 17 Oct 23 10:41 IST |                     |
| service    | list                | minikube | akshay | v1.31.2 | 17 Oct 23 10:43 IST | 17 Oct 23 10:43 IST |
| service    | teeverse            | minikube | akshay | v1.31.2 | 17 Oct 23 10:48 IST | 17 Oct 23 10:48 IST |
| service    | teeverse            | minikube | akshay | v1.31.2 | 17 Oct 23 10:54 IST | 17 Oct 23 10:54 IST |
| service    | teeverse            | minikube | akshay | v1.31.2 | 17 Oct 23 10:59 IST | 17 Oct 23 10:59 IST |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 11:00 IST | 17 Oct 23 11:00 IST |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 11:34 IST | 17 Oct 23 11:35 IST |
| service    | list                | minikube | akshay | v1.31.2 | 17 Oct 23 11:42 IST | 17 Oct 23 11:42 IST |
| service    | teeverse-service    | minikube | akshay | v1.31.2 | 17 Oct 23 11:43 IST |                     |
| service    | list                | minikube | akshay | v1.31.2 | 17 Oct 23 11:54 IST | 17 Oct 23 11:54 IST |
| service    | list                | minikube | akshay | v1.31.2 | 17 Oct 23 13:28 IST | 17 Oct 23 13:28 IST |
| service    | list                | minikube | akshay | v1.31.2 | 17 Oct 23 16:35 IST | 17 Oct 23 16:35 IST |
| service    | teeverse-service    | minikube | akshay | v1.31.2 | 17 Oct 23 16:35 IST |                     |
| ssh        |                     | minikube | akshay | v1.31.2 | 17 Oct 23 17:11 IST | 17 Oct 23 17:13 IST |
| service    | list                | minikube | akshay | v1.31.2 | 17 Oct 23 17:20 IST | 17 Oct 23 17:20 IST |
| service    | teeverse            | minikube | akshay | v1.31.2 | 17 Oct 23 17:20 IST | 17 Oct 23 17:20 IST |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 17:24 IST | 17 Oct 23 17:24 IST |
| ssh        |                     | minikube | akshay | v1.31.2 | 17 Oct 23 17:53 IST |                     |
| ssh        |                     | minikube | akshay | v1.31.2 | 17 Oct 23 17:58 IST |                     |
| service    | list                | minikube | akshay | v1.31.2 | 17 Oct 23 18:13 IST | 17 Oct 23 18:13 IST |
| docker-env |                     | minikube | akshay | v1.31.2 | 17 Oct 23 18:39 IST | 17 Oct 23 18:39 IST |
| ssh        |                     | minikube | akshay | v1.31.2 | 17 Oct 23 18:39 IST | 17 Oct 23 18:41 IST |
| ssh        |                     | minikube | akshay | v1.31.2 | 17 Oct 23 18:51 IST | 17 Oct 23 18:52 IST |
| ssh        |                     | minikube | akshay | v1.31.2 | 17 Oct 23 18:57 IST | 17 Oct 23 18:59 IST |
| ip         |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:03 IST | 17 Oct 23 19:03 IST |
| stop       |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:44 IST | 17 Oct 23 19:44 IST |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:44 IST | 17 Oct 23 19:45 IST |
| ip         |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:49 IST | 17 Oct 23 19:49 IST |
| ssh        |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:50 IST |                     |
| stop       |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:51 IST |                     |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:51 IST |                     |
| ip         |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:51 IST |                     |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:52 IST |                     |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 19:53 IST |                     |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 20:08 IST |                     |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 20:12 IST |                     |
| delete     |                     | minikube | akshay | v1.31.2 | 17 Oct 23 20:21 IST | 17 Oct 23 20:22 IST |
| start      |                     | minikube | akshay | v1.31.2 | 17 Oct 23 20:22 IST | 17 Oct 23 20:26 IST |
|------------|---------------------|----------|--------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/10/17 20:22:37
Running on machine: akshay-HP-Laptop-15s-fq2xxx
Binary: Built with gc go1.20.7 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1017 20:22:37.419831  579801 out.go:296] Setting OutFile to fd 1 ...
I1017 20:22:37.420026  579801 out.go:348] isatty.IsTerminal(1) = true
I1017 20:22:37.420031  579801 out.go:309] Setting ErrFile to fd 2...
I1017 20:22:37.420039  579801 out.go:348] isatty.IsTerminal(2) = true
I1017 20:22:37.420368  579801 root.go:338] Updating PATH: /home/akshay/.minikube/bin
I1017 20:22:37.420387  579801 oci.go:573] shell is pointing to dockerd inside minikube. will unset to use host
I1017 20:22:37.421072  579801 out.go:303] Setting JSON to false
I1017 20:22:37.423229  579801 start.go:128] hostinfo: {"hostname":"akshay-HP-Laptop-15s-fq2xxx","uptime":41765,"bootTime":1697512592,"procs":361,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.2.0-34-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"3ad25de9-2c85-4ac5-914c-aaeac8e7b84a"}
I1017 20:22:37.423318  579801 start.go:138] virtualization: kvm host
I1017 20:22:37.425002  579801 out.go:177] üòÑ  minikube v1.31.2 on Ubuntu 22.04
I1017 20:22:37.426309  579801 out.go:177]     ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
I1017 20:22:37.426307  579801 notify.go:220] Checking for updates...
I1017 20:22:37.427253  579801 driver.go:373] Setting default libvirt URI to qemu:///system
I1017 20:22:37.427280  579801 global.go:111] Querying for installed drivers using PATH=/home/akshay/.minikube/bin:/home/akshay/akshay/Apps/app1/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/usr/local/go:/usr/local/go/bin
I1017 20:22:37.443965  579801 global.go:122] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1017 20:22:37.444666  579801 global.go:122] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1017 20:22:37.444919  579801 global.go:122] qemu2 default: true priority: 7, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1017 20:22:37.444932  579801 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1017 20:22:37.445015  579801 global.go:122] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1017 20:22:37.445259  579801 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
W1017 20:22:43.448568  579801 docker.go:168] docker version returned error: deadline exceeded running "docker version --format {{.Server.Os}}-{{.Server.Version}}:{{.Server.Platform.Name}}": signal: killed
I1017 20:22:43.448637  579801 global.go:122] docker default: true priority: 9, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:signal: killed
deadline exceeded running "docker version --format {{.Server.Os}}-{{.Server.Version}}:{{.Server.Platform.Name}}"
k8s.io/minikube/pkg/minikube/registry/drvs/docker.glob..func1
	/app/pkg/minikube/registry/drvs/docker/docker.go:164
k8s.io/minikube/pkg/minikube/registry/drvs/docker.status
	/app/pkg/minikube/registry/drvs/docker/docker.go:97
k8s.io/minikube/pkg/minikube/registry.Available
	/app/pkg/minikube/registry/global.go:121
k8s.io/minikube/pkg/minikube/driver.Choices
	/app/pkg/minikube/driver/driver.go:285
k8s.io/minikube/cmd/minikube/cmd.selectDriver
	/app/cmd/minikube/cmd/start.go:727
k8s.io/minikube/cmd/minikube/cmd.runStart
	/app/cmd/minikube/cmd/start.go:207
github.com/spf13/cobra.(*Command).execute
	/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:944
github.com/spf13/cobra.(*Command).ExecuteC
	/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068
github.com/spf13/cobra.(*Command).Execute
	/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992
k8s.io/minikube/cmd/minikube/cmd.Execute
	/app/cmd/minikube/cmd/root.go:174
main.main
	/app/cmd/minikube/main.go:95
runtime.main
	/usr/local/go/src/runtime/proc.go:250
runtime.goexit
	/usr/local/go/src/runtime/asm_amd64.s:1598 Reason:PROVIDER_DOCKER_DEADLINE_EXCEEDED Fix:Restart the Docker service Doc:https://minikube.sigs.k8s.io/docs/drivers/docker/ Version:}
I1017 20:22:43.511656  579801 global.go:122] kvm2 default: true priority: 8, state: {Installed:true Healthy:false Running:true NeedsImprovement:false Error:/usr/bin/virsh domcapabilities --virttype kvm failed:
error: failed to connect to the hypervisor
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied
exit status 1 Reason: Fix:Follow your Linux distribution instructions for configuring KVM Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1017 20:22:43.511734  579801 driver.go:308] not recommending "none" due to default: false
I1017 20:22:43.511751  579801 driver.go:308] not recommending "ssh" due to default: false
I1017 20:22:43.511762  579801 driver.go:303] not recommending "docker" due to health: deadline exceeded running "docker version --format {{.Server.Os}}-{{.Server.Version}}:{{.Server.Platform.Name}}": signal: killed
I1017 20:22:43.511778  579801 driver.go:303] not recommending "kvm2" due to health: /usr/bin/virsh domcapabilities --virttype kvm failed:
error: failed to connect to the hypervisor
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied
exit status 1
I1017 20:22:43.511821  579801 driver.go:343] Picked: qemu2
I1017 20:22:43.511839  579801 driver.go:344] Alternatives: [none ssh]
I1017 20:22:43.511853  579801 driver.go:345] Rejects: [podman virtualbox vmware docker kvm2]
I1017 20:22:43.513078  579801 out.go:177] ‚ú®  Automatically selected the qemu2 driver. Other choices: none, ssh
I1017 20:22:43.513855  579801 start.go:298] selected driver: qemu2
I1017 20:22:43.513868  579801 start.go:902] validating driver "qemu2" against <nil>
I1017 20:22:43.513889  579801 start.go:913] status for qemu2: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1017 20:22:43.514785  579801 start_flags.go:305] no existing cluster config was found, will generate one from the flags 
I1017 20:22:43.515608  579801 out.go:177] üåê  Automatically selected the builtin network
W1017 20:22:43.516347  579801 out.go:239] ‚ùó  You are using the QEMU driver without a dedicated network, which doesn't support `minikube service` & `minikube tunnel` commands.
I1017 20:22:43.517783  579801 start_flags.go:382] Using suggested 2200MB memory alloc based on sys=7577MB, container=0MB
I1017 20:22:43.518143  579801 start_flags.go:901] Wait components to verify : map[apiserver:true system_pods:true]
I1017 20:22:43.518188  579801 cni.go:84] Creating CNI manager for ""
I1017 20:22:43.518216  579801 cni.go:158] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1017 20:22:43.518228  579801 start_flags.go:314] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1017 20:22:43.518242  579801 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:builtin Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/akshay:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1017 20:22:43.518795  579801 iso.go:125] acquiring lock: {Name:mkd09348baa7fd95131cb9da8621b980e6e1d9b6 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1017 20:22:43.519791  579801 out.go:177] üíø  Downloading VM boot image ...
I1017 20:22:43.520550  579801 download.go:107] Downloading: https://storage.googleapis.com/minikube-builds/iso/16971/minikube-v1.31.0-amd64.iso?checksum=file:https://storage.googleapis.com/minikube-builds/iso/16971/minikube-v1.31.0-amd64.iso.sha256 -> /home/akshay/.minikube/cache/iso/amd64/minikube-v1.31.0-amd64.iso
E1017 20:22:44.906271  579801 iso.go:90] Unable to download https://storage.googleapis.com/minikube-builds/iso/16971/minikube-v1.31.0-amd64.iso: getter: &{Ctx:context.Background Src:https://storage.googleapis.com/minikube-builds/iso/16971/minikube-v1.31.0-amd64.iso?checksum=file:https://storage.googleapis.com/minikube-builds/iso/16971/minikube-v1.31.0-amd64.iso.sha256 Dst:/home/akshay/.minikube/cache/iso/amd64/minikube-v1.31.0-amd64.iso.download Pwd: Mode:2 Umask:---------- Detectors:[0x3f9c8a8 0x3f9c8a8 0x3f9c8a8 0x3f9c8a8 0x3f9c8a8 0x3f9c8a8 0x3f9c8a8] Decompressors:map[bz2:0xc0004bfd28 gz:0xc0004bfd90 tar:0xc0004bfd30 tar.bz2:0xc0004bfd40 tar.gz:0xc0004bfd50 tar.xz:0xc0004bfd60 tar.zst:0xc0004bfd80 tbz2:0xc0004bfd40 tgz:0xc0004bfd50 txz:0xc0004bfd60 tzst:0xc0004bfd80 xz:0xc0004bfd98 zip:0xc0004bfda0 zst:0xc0004bfdc0] Getters:map[file:0xc00121df60 http:0xc001258320 https:0xc001258370] Dir:false ProgressListener:0x3f579a0 Insecure:false DisableSymlinks:false Options:[0x12d0880]}: invalid checksum: Error downloading checksum file: bad response code: 404
I1017 20:22:44.906347  579801 iso.go:125] acquiring lock: {Name:mkd09348baa7fd95131cb9da8621b980e6e1d9b6 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1017 20:22:44.907731  579801 out.go:177] üíø  Downloading VM boot image ...
I1017 20:22:44.908762  579801 download.go:107] Downloading: https://github.com/kubernetes/minikube/releases/download/v1.31.0/minikube-v1.31.0-amd64.iso?checksum=file:https://github.com/kubernetes/minikube/releases/download/v1.31.0/minikube-v1.31.0-amd64.iso.sha256 -> /home/akshay/.minikube/cache/iso/amd64/minikube-v1.31.0-amd64.iso
I1017 20:24:13.166605  579801 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1017 20:24:13.167585  579801 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1017 20:24:13.167631  579801 preload.go:148] Found local preload: /home/akshay/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1017 20:24:13.167642  579801 cache.go:57] Caching tarball of preloaded images
I1017 20:24:13.167802  579801 preload.go:174] Found /home/akshay/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1017 20:24:13.167815  579801 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I1017 20:24:13.168462  579801 profile.go:148] Saving config to /home/akshay/.minikube/profiles/minikube/config.json ...
I1017 20:24:13.168496  579801 lock.go:35] WriteFile acquiring /home/akshay/.minikube/profiles/minikube/config.json: {Name:mk7f5035e26ae6bf1f06742024a95489b87e6745 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:24:13.168714  579801 start.go:365] acquiring machines lock for minikube: {Name:mk48281e7404f7688b366008fc8e2f16830f27fb Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I1017 20:24:13.168809  579801 start.go:369] acquired machines lock for "minikube" in 79.535¬µs
I1017 20:24:13.168827  579801 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://github.com/kubernetes/minikube/releases/download/v1.31.0/minikube-v1.31.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:builtin Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/akshay:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0} &{Name: IP: Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1017 20:24:13.168941  579801 start.go:125] createHost starting for "" (driver="qemu2")
I1017 20:24:13.169865  579801 out.go:204] üî•  Creating qemu2 VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
I1017 20:24:13.170494  579801 start.go:159] libmachine.API.Create for "minikube" (driver="qemu2")
I1017 20:24:13.170529  579801 client.go:168] LocalClient.Create starting
I1017 20:24:13.170772  579801 main.go:141] libmachine: Reading certificate data from /home/akshay/.minikube/certs/ca.pem
I1017 20:24:13.170822  579801 main.go:141] libmachine: Decoding PEM data...
I1017 20:24:13.170847  579801 main.go:141] libmachine: Parsing certificate...
I1017 20:24:13.170939  579801 main.go:141] libmachine: Reading certificate data from /home/akshay/.minikube/certs/cert.pem
I1017 20:24:13.170967  579801 main.go:141] libmachine: Decoding PEM data...
I1017 20:24:13.170986  579801 main.go:141] libmachine: Parsing certificate...
I1017 20:24:13.171536  579801 main.go:141] libmachine: port range: 0 -> 65535
I1017 20:24:13.171846  579801 main.go:141] libmachine: Downloading /home/akshay/.minikube/cache/boot2docker.iso from file:///home/akshay/.minikube/cache/iso/amd64/minikube-v1.31.0-amd64.iso...
I1017 20:24:13.664583  579801 main.go:141] libmachine: Creating SSH key...
I1017 20:24:14.249450  579801 main.go:141] libmachine: Creating Disk image...
I1017 20:24:14.249470  579801 main.go:141] libmachine: Creating 20000 MB hard disk image...
I1017 20:24:14.249662  579801 main.go:141] libmachine: executing: qemu-img convert -f raw -O qcow2 /home/akshay/.minikube/machines/minikube/disk.qcow2.raw /home/akshay/.minikube/machines/minikube/disk.qcow2
I1017 20:24:14.266271  579801 main.go:141] libmachine: STDOUT: 
I1017 20:24:14.266290  579801 main.go:141] libmachine: STDERR: 
I1017 20:24:14.266351  579801 main.go:141] libmachine: executing: qemu-img resize /home/akshay/.minikube/machines/minikube/disk.qcow2 +20000M
I1017 20:24:14.275786  579801 main.go:141] libmachine: STDOUT: Image resized.

I1017 20:24:14.275811  579801 main.go:141] libmachine: STDERR: 
I1017 20:24:14.275841  579801 main.go:141] libmachine: DONE writing to /home/akshay/.minikube/machines/minikube/disk.qcow2.raw and /home/akshay/.minikube/machines/minikube/disk.qcow2
I1017 20:24:14.275853  579801 main.go:141] libmachine: Starting QEMU VM...
I1017 20:24:14.275960  579801 main.go:141] libmachine: executing: qemu-system-x86_64 -display none -accel kvm -m 2200 -smp 2 -boot d -cdrom /home/akshay/.minikube/machines/minikube/boot2docker.iso -qmp unix:/home/akshay/.minikube/machines/minikube/monitor,server,nowait -pidfile /home/akshay/.minikube/machines/minikube/qemu.pid -nic user,model=virtio,hostfwd=tcp::41589-:22,hostfwd=tcp::35321-:2376,hostname=minikube -daemonize /home/akshay/.minikube/machines/minikube/disk.qcow2
I1017 20:24:14.443133  579801 main.go:141] libmachine: STDOUT: 
W1017 20:24:14.443167  579801 main.go:139] libmachine: STDERR: qemu-system-x86_64: warning: host doesn't support requested feature: CPUID.80000001H:ECX.svm [bit 2]
qemu-system-x86_64: warning: host doesn't support requested feature: CPUID.80000001H:ECX.svm [bit 2]

I1017 20:24:14.443200  579801 main.go:141] libmachine: Waiting for VM to start (ssh -p 41589 docker@127.0.0.1)...
I1017 20:25:25.135014  579801 machine.go:88] provisioning docker machine ...
I1017 20:25:25.136324  579801 buildroot.go:166] provisioning hostname "minikube"
I1017 20:25:25.137483  579801 main.go:141] libmachine: Using SSH client type: native
I1017 20:25:25.141159  579801 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} localhost 41589 <nil> <nil>}
I1017 20:25:25.141174  579801 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1017 20:25:25.280152  579801 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1017 20:25:25.280248  579801 main.go:141] libmachine: Using SSH client type: native
I1017 20:25:25.281318  579801 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} localhost 41589 <nil> <nil>}
I1017 20:25:25.281341  579801 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1017 20:25:25.399841  579801 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1017 20:25:25.399864  579801 buildroot.go:172] set auth options {CertDir:/home/akshay/.minikube CaCertPath:/home/akshay/.minikube/certs/ca.pem CaPrivateKeyPath:/home/akshay/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/akshay/.minikube/machines/server.pem ServerKeyPath:/home/akshay/.minikube/machines/server-key.pem ClientKeyPath:/home/akshay/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/akshay/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/akshay/.minikube}
I1017 20:25:25.399894  579801 buildroot.go:174] setting up certificates
I1017 20:25:25.404261  579801 provision.go:83] configureAuth start
I1017 20:25:25.404286  579801 provision.go:138] copyHostCerts
I1017 20:25:25.408111  579801 exec_runner.go:144] found /home/akshay/.minikube/ca.pem, removing ...
I1017 20:25:25.408353  579801 exec_runner.go:203] rm: /home/akshay/.minikube/ca.pem
I1017 20:25:25.409125  579801 exec_runner.go:151] cp: /home/akshay/.minikube/certs/ca.pem --> /home/akshay/.minikube/ca.pem (1078 bytes)
I1017 20:25:25.409618  579801 exec_runner.go:144] found /home/akshay/.minikube/cert.pem, removing ...
I1017 20:25:25.409626  579801 exec_runner.go:203] rm: /home/akshay/.minikube/cert.pem
I1017 20:25:25.409668  579801 exec_runner.go:151] cp: /home/akshay/.minikube/certs/cert.pem --> /home/akshay/.minikube/cert.pem (1123 bytes)
I1017 20:25:25.409804  579801 exec_runner.go:144] found /home/akshay/.minikube/key.pem, removing ...
I1017 20:25:25.409812  579801 exec_runner.go:203] rm: /home/akshay/.minikube/key.pem
I1017 20:25:25.410047  579801 exec_runner.go:151] cp: /home/akshay/.minikube/certs/key.pem --> /home/akshay/.minikube/key.pem (1675 bytes)
I1017 20:25:25.410214  579801 provision.go:112] generating server cert: /home/akshay/.minikube/machines/server.pem ca-key=/home/akshay/.minikube/certs/ca.pem private-key=/home/akshay/.minikube/certs/ca-key.pem org=akshay.minikube san=[127.0.0.1 localhost localhost 127.0.0.1 minikube minikube]
I1017 20:25:25.634347  579801 provision.go:172] copyRemoteCerts
I1017 20:25:25.635641  579801 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1017 20:25:25.635660  579801 sshutil.go:53] new ssh client: &{IP:localhost Port:41589 SSHKeyPath:/home/akshay/.minikube/machines/minikube/id_rsa Username:docker}
I1017 20:25:25.707981  579801 ssh_runner.go:362] scp /home/akshay/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1017 20:25:25.749154  579801 ssh_runner.go:362] scp /home/akshay/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I1017 20:25:25.787588  579801 ssh_runner.go:362] scp /home/akshay/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1017 20:25:25.821820  579801 provision.go:86] duration metric: configureAuth took 417.5112ms
I1017 20:25:25.821848  579801 buildroot.go:189] setting minikube options for container-runtime
I1017 20:25:25.822442  579801 config.go:182] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1017 20:25:25.822732  579801 main.go:141] libmachine: Using SSH client type: native
I1017 20:25:25.823454  579801 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} localhost 41589 <nil> <nil>}
I1017 20:25:25.823466  579801 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1017 20:25:25.938704  579801 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I1017 20:25:25.938723  579801 buildroot.go:70] root file system type: tmpfs
I1017 20:25:25.951788  579801 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1017 20:25:25.951853  579801 main.go:141] libmachine: Using SSH client type: native
I1017 20:25:25.952590  579801 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} localhost 41589 <nil> <nil>}
I1017 20:25:25.952694  579801 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1017 20:25:26.089802  579801 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1017 20:25:26.090629  579801 main.go:141] libmachine: Using SSH client type: native
I1017 20:25:26.091433  579801 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} localhost 41589 <nil> <nil>}
I1017 20:25:26.091456  579801 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1017 20:25:27.433285  579801 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I1017 20:25:27.433305  579801 machine.go:91] provisioned docker machine in 2.298267574s
I1017 20:25:27.433317  579801 client.go:171] LocalClient.Create took 1m14.262780887s
I1017 20:25:27.433336  579801 start.go:167] duration metric: libmachine.API.Create for "minikube" took 1m14.262843101s
I1017 20:25:27.433360  579801 start.go:300] post-start starting for "minikube" (driver="qemu2")
I1017 20:25:27.433371  579801 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1017 20:25:27.433459  579801 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1017 20:25:27.433476  579801 sshutil.go:53] new ssh client: &{IP:localhost Port:41589 SSHKeyPath:/home/akshay/.minikube/machines/minikube/id_rsa Username:docker}
I1017 20:25:27.506839  579801 ssh_runner.go:195] Run: cat /etc/os-release
I1017 20:25:27.512863  579801 info.go:137] Remote host: Buildroot 2021.02.12
I1017 20:25:27.512880  579801 filesync.go:126] Scanning /home/akshay/.minikube/addons for local assets ...
I1017 20:25:27.513198  579801 filesync.go:126] Scanning /home/akshay/.minikube/files for local assets ...
I1017 20:25:27.513329  579801 start.go:303] post-start completed in 79.962956ms
I1017 20:25:27.514169  579801 profile.go:148] Saving config to /home/akshay/.minikube/profiles/minikube/config.json ...
I1017 20:25:27.514527  579801 start.go:128] duration metric: createHost completed in 1m14.345576726s
I1017 20:25:27.514580  579801 main.go:141] libmachine: Using SSH client type: native
I1017 20:25:27.515304  579801 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} localhost 41589 <nil> <nil>}
I1017 20:25:27.515315  579801 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I1017 20:25:27.641254  579801 main.go:141] libmachine: SSH cmd err, output: <nil>: 1697554527.634418719

I1017 20:25:27.641267  579801 fix.go:206] guest clock: 1697554527.634418719
I1017 20:25:27.641282  579801 fix.go:219] Guest: 2023-10-17 20:25:27.634418719 +0530 IST Remote: 2023-10-17 20:25:27.514532811 +0530 IST m=+170.153009180 (delta=119.885908ms)
I1017 20:25:27.641331  579801 fix.go:190] guest clock delta is within tolerance: 119.885908ms
I1017 20:25:27.641345  579801 start.go:83] releasing machines lock for "minikube", held for 1m14.472519474s
I1017 20:25:27.642071  579801 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1017 20:25:27.642649  579801 sshutil.go:53] new ssh client: &{IP:localhost Port:41589 SSHKeyPath:/home/akshay/.minikube/machines/minikube/id_rsa Username:docker}
I1017 20:25:27.642792  579801 ssh_runner.go:195] Run: cat /version.json
I1017 20:25:27.642805  579801 sshutil.go:53] new ssh client: &{IP:localhost Port:41589 SSHKeyPath:/home/akshay/.minikube/machines/minikube/id_rsa Username:docker}
I1017 20:25:28.166645  579801 ssh_runner.go:195] Run: systemctl --version
I1017 20:25:28.175153  579801 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W1017 20:25:28.183280  579801 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I1017 20:25:28.183373  579801 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1017 20:25:28.205726  579801 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1017 20:25:28.206009  579801 start.go:466] detecting cgroup driver to use...
I1017 20:25:28.206435  579801 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1017 20:25:28.241387  579801 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1017 20:25:28.257245  579801 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1017 20:25:28.273858  579801 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1017 20:25:28.273922  579801 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1017 20:25:28.289593  579801 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1017 20:25:28.305338  579801 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1017 20:25:28.321402  579801 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1017 20:25:28.337322  579801 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1017 20:25:28.353116  579801 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1017 20:25:28.369036  579801 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1017 20:25:28.383450  579801 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1017 20:25:28.397596  579801 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1017 20:25:28.728605  579801 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1017 20:25:28.754578  579801 start.go:466] detecting cgroup driver to use...
I1017 20:25:28.754676  579801 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1017 20:25:28.780251  579801 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1017 20:25:28.800982  579801 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1017 20:25:28.834970  579801 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1017 20:25:28.854633  579801 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1017 20:25:28.876431  579801 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I1017 20:25:29.007224  579801 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1017 20:25:29.029170  579801 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1017 20:25:29.057015  579801 ssh_runner.go:195] Run: which cri-dockerd
I1017 20:25:29.062909  579801 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1017 20:25:29.078334  579801 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1017 20:25:29.104881  579801 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1017 20:25:29.352265  579801 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1017 20:25:29.581004  579801 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I1017 20:25:29.581043  579801 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I1017 20:25:29.606101  579801 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1017 20:25:29.847605  579801 ssh_runner.go:195] Run: sudo systemctl restart docker
I1017 20:25:31.429369  579801 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.581729232s)
I1017 20:25:31.429449  579801 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1017 20:25:31.668678  579801 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1017 20:25:31.906890  579801 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1017 20:25:32.153992  579801 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1017 20:25:32.388947  579801 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1017 20:25:32.417543  579801 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1017 20:25:32.702904  579801 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1017 20:25:32.880145  579801 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1017 20:25:32.880242  579801 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1017 20:25:32.892886  579801 start.go:534] Will wait 60s for crictl version
I1017 20:25:32.892988  579801 ssh_runner.go:195] Run: which crictl
I1017 20:25:32.901475  579801 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1017 20:25:32.995847  579801 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1alpha2
I1017 20:25:32.995967  579801 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1017 20:25:33.059341  579801 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1017 20:25:33.131979  579801 out.go:204] üê≥  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I1017 20:25:33.133020  579801 ssh_runner.go:195] Run: grep 10.0.2.2	host.minikube.internal$ /etc/hosts
I1017 20:25:33.141269  579801 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "10.0.2.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1017 20:25:33.167338  579801 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1017 20:25:33.167452  579801 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1017 20:25:33.197047  579801 docker.go:636] Got preloaded images: 
I1017 20:25:33.197069  579801 docker.go:642] registry.k8s.io/kube-apiserver:v1.27.4 wasn't preloaded
I1017 20:25:33.197161  579801 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I1017 20:25:33.211845  579801 ssh_runner.go:195] Run: which lz4
I1017 20:25:33.217506  579801 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /preloaded.tar.lz4
I1017 20:25:33.224127  579801 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%!s(MISSING) %!y(MISSING)" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I1017 20:25:33.224170  579801 ssh_runner.go:362] scp /home/akshay/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (412313883 bytes)
I1017 20:25:44.052865  579801 docker.go:600] Took 10.832583 seconds to copy over tarball
I1017 20:25:44.055453  579801 ssh_runner.go:195] Run: sudo tar -I lz4 -C /var -xf /preloaded.tar.lz4
I1017 20:25:53.633105  579801 ssh_runner.go:235] Completed: sudo tar -I lz4 -C /var -xf /preloaded.tar.lz4: (9.573511073s)
I1017 20:25:53.634233  579801 ssh_runner.go:146] rm: /preloaded.tar.lz4
I1017 20:25:53.701345  579801 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I1017 20:25:53.717130  579801 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2629 bytes)
I1017 20:25:53.741431  579801 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1017 20:25:53.962957  579801 ssh_runner.go:195] Run: sudo systemctl restart docker
I1017 20:25:55.870856  579801 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.904885352s)
I1017 20:25:55.876952  579801 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1017 20:25:55.912473  579801 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1017 20:25:55.913646  579801 cache_images.go:84] Images are preloaded, skipping loading
I1017 20:25:55.914611  579801 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1017 20:25:55.963520  579801 cni.go:84] Creating CNI manager for ""
I1017 20:25:55.963546  579801 cni.go:158] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1017 20:25:55.964586  579801 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1017 20:25:55.964658  579801 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:10.0.2.15 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "10.0.2.15"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:10.0.2.15 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1017 20:25:55.967267  579801 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.2.15
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 10.0.2.15
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "10.0.2.15"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1017 20:25:55.969335  579801 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=10.0.2.15

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1017 20:25:55.969600  579801 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I1017 20:25:55.988542  579801 binaries.go:44] Found k8s binaries, skipping transfer
I1017 20:25:55.988847  579801 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1017 20:25:56.010552  579801 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (366 bytes)
I1017 20:25:56.043965  579801 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1017 20:25:56.076031  579801 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2082 bytes)
I1017 20:25:56.100793  579801 ssh_runner.go:195] Run: grep 10.0.2.15	control-plane.minikube.internal$ /etc/hosts
I1017 20:25:56.106516  579801 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "10.0.2.15	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1017 20:25:56.125830  579801 certs.go:56] Setting up /home/akshay/.minikube/profiles/minikube for IP: 10.0.2.15
I1017 20:25:56.125859  579801 certs.go:190] acquiring lock for shared ca certs: {Name:mkee17142ac81ea78212379dc1c3080d975dbc2f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:25:56.130326  579801 certs.go:199] skipping minikubeCA CA generation: /home/akshay/.minikube/ca.key
I1017 20:25:56.130497  579801 certs.go:199] skipping proxyClientCA CA generation: /home/akshay/.minikube/proxy-client-ca.key
I1017 20:25:56.130669  579801 certs.go:319] generating minikube-user signed cert: /home/akshay/.minikube/profiles/minikube/client.key
I1017 20:25:56.131148  579801 crypto.go:68] Generating cert /home/akshay/.minikube/profiles/minikube/client.crt with IP's: []
I1017 20:25:56.367691  579801 crypto.go:156] Writing cert to /home/akshay/.minikube/profiles/minikube/client.crt ...
I1017 20:25:56.367744  579801 lock.go:35] WriteFile acquiring /home/akshay/.minikube/profiles/minikube/client.crt: {Name:mk5a9ae62522d7c7d996d3bee1398924b307b827 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:25:56.371008  579801 crypto.go:164] Writing key to /home/akshay/.minikube/profiles/minikube/client.key ...
I1017 20:25:56.371032  579801 lock.go:35] WriteFile acquiring /home/akshay/.minikube/profiles/minikube/client.key: {Name:mked2b3d002d91c3fe2ce670949c71defa970ee6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:25:56.371211  579801 certs.go:319] generating minikube signed cert: /home/akshay/.minikube/profiles/minikube/apiserver.key.49504c3e
I1017 20:25:56.371235  579801 crypto.go:68] Generating cert /home/akshay/.minikube/profiles/minikube/apiserver.crt.49504c3e with IP's: [10.0.2.15 10.96.0.1 127.0.0.1 10.0.0.1]
I1017 20:25:57.312788  579801 crypto.go:156] Writing cert to /home/akshay/.minikube/profiles/minikube/apiserver.crt.49504c3e ...
I1017 20:25:57.318210  579801 lock.go:35] WriteFile acquiring /home/akshay/.minikube/profiles/minikube/apiserver.crt.49504c3e: {Name:mk86db41d8d402ceaed048d51edce8b5c4054b7b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:25:57.325277  579801 crypto.go:164] Writing key to /home/akshay/.minikube/profiles/minikube/apiserver.key.49504c3e ...
I1017 20:25:57.325300  579801 lock.go:35] WriteFile acquiring /home/akshay/.minikube/profiles/minikube/apiserver.key.49504c3e: {Name:mk1b161d812f1ea4bf7b6c58b95ec5405585ebeb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:25:57.325882  579801 certs.go:337] copying /home/akshay/.minikube/profiles/minikube/apiserver.crt.49504c3e -> /home/akshay/.minikube/profiles/minikube/apiserver.crt
I1017 20:25:57.330114  579801 certs.go:341] copying /home/akshay/.minikube/profiles/minikube/apiserver.key.49504c3e -> /home/akshay/.minikube/profiles/minikube/apiserver.key
I1017 20:25:57.330575  579801 certs.go:319] generating aggregator signed cert: /home/akshay/.minikube/profiles/minikube/proxy-client.key
I1017 20:25:57.330908  579801 crypto.go:68] Generating cert /home/akshay/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1017 20:25:58.012469  579801 crypto.go:156] Writing cert to /home/akshay/.minikube/profiles/minikube/proxy-client.crt ...
I1017 20:25:58.012498  579801 lock.go:35] WriteFile acquiring /home/akshay/.minikube/profiles/minikube/proxy-client.crt: {Name:mk670a04755b1851deda14d2ba9251a0dbe296d4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:25:58.013010  579801 crypto.go:164] Writing key to /home/akshay/.minikube/profiles/minikube/proxy-client.key ...
I1017 20:25:58.013025  579801 lock.go:35] WriteFile acquiring /home/akshay/.minikube/profiles/minikube/proxy-client.key: {Name:mkd1b2cca9f6d7b0df068a1d42542f2220b58014 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:25:58.015345  579801 certs.go:437] found cert: /home/akshay/.minikube/certs/home/akshay/.minikube/certs/ca-key.pem (1675 bytes)
I1017 20:25:58.015511  579801 certs.go:437] found cert: /home/akshay/.minikube/certs/home/akshay/.minikube/certs/ca.pem (1078 bytes)
I1017 20:25:58.015918  579801 certs.go:437] found cert: /home/akshay/.minikube/certs/home/akshay/.minikube/certs/cert.pem (1123 bytes)
I1017 20:25:58.016262  579801 certs.go:437] found cert: /home/akshay/.minikube/certs/home/akshay/.minikube/certs/key.pem (1675 bytes)
I1017 20:25:58.029495  579801 ssh_runner.go:362] scp /home/akshay/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1017 20:25:58.069516  579801 ssh_runner.go:362] scp /home/akshay/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1017 20:25:58.109733  579801 ssh_runner.go:362] scp /home/akshay/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1017 20:25:58.147100  579801 ssh_runner.go:362] scp /home/akshay/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1017 20:25:58.180092  579801 ssh_runner.go:362] scp /home/akshay/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1017 20:25:58.212857  579801 ssh_runner.go:362] scp /home/akshay/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1017 20:25:58.245373  579801 ssh_runner.go:362] scp /home/akshay/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1017 20:25:58.278157  579801 ssh_runner.go:362] scp /home/akshay/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1017 20:25:58.311345  579801 ssh_runner.go:362] scp /home/akshay/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1017 20:25:58.344217  579801 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1017 20:25:58.377147  579801 ssh_runner.go:195] Run: openssl version
I1017 20:25:58.388935  579801 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1017 20:25:58.410654  579801 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1017 20:25:58.420312  579801 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Oct 17 04:14 /usr/share/ca-certificates/minikubeCA.pem
I1017 20:25:58.420392  579801 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1017 20:25:58.431534  579801 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1017 20:25:58.454379  579801 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1017 20:25:58.462111  579801 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I1017 20:25:58.468045  579801 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://github.com/kubernetes/minikube/releases/download/v1.31.0/minikube-v1.31.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:36739 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:10.0.2.15 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:builtin Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/akshay:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1017 20:25:58.476574  579801 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1017 20:25:58.507047  579801 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1017 20:25:58.522574  579801 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1017 20:25:58.537374  579801 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1017 20:25:58.551455  579801 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1017 20:25:58.551781  579801 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I1017 20:25:58.968043  579801 kubeadm.go:322] [init] Using Kubernetes version: v1.27.4
I1017 20:25:58.968217  579801 kubeadm.go:322] [preflight] Running pre-flight checks
I1017 20:25:59.859290  579801 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I1017 20:25:59.859608  579801 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1017 20:25:59.859859  579801 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1017 20:26:00.189296  579801 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1017 20:26:00.195610  579801 out.go:204]     ‚ñ™ Generating certificates and keys ...
I1017 20:26:00.197702  579801 kubeadm.go:322] [certs] Using existing ca certificate authority
I1017 20:26:00.197882  579801 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I1017 20:26:00.553964  579801 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I1017 20:26:01.394538  579801 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I1017 20:26:02.124434  579801 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I1017 20:26:02.497221  579801 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I1017 20:26:02.889513  579801 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I1017 20:26:02.889876  579801 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [10.0.2.15 127.0.0.1 ::1]
I1017 20:26:03.220707  579801 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I1017 20:26:03.221052  579801 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [10.0.2.15 127.0.0.1 ::1]
I1017 20:26:03.575621  579801 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I1017 20:26:03.995269  579801 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I1017 20:26:05.146918  579801 kubeadm.go:322] [certs] Generating "sa" key and public key
I1017 20:26:05.147401  579801 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1017 20:26:05.968385  579801 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I1017 20:26:07.435375  579801 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1017 20:26:07.854413  579801 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1017 20:26:08.631037  579801 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1017 20:26:08.662295  579801 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1017 20:26:08.663901  579801 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1017 20:26:08.664000  579801 kubeadm.go:322] [kubelet-start] Starting the kubelet
I1017 20:26:08.930145  579801 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1017 20:26:08.936362  579801 out.go:204]     ‚ñ™ Booting up control plane ...
I1017 20:26:08.938558  579801 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1017 20:26:08.938820  579801 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1017 20:26:08.941126  579801 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1017 20:26:08.943488  579801 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1017 20:26:08.962980  579801 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I1017 20:26:23.480724  579801 kubeadm.go:322] [apiclient] All control plane components are healthy after 14.507993 seconds
I1017 20:26:23.481590  579801 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1017 20:26:23.516782  579801 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1017 20:26:24.139357  579801 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I1017 20:26:24.139865  579801 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1017 20:26:24.706279  579801 kubeadm.go:322] [bootstrap-token] Using token: vn2rep.6qrku7ag74yp5z7c
I1017 20:26:24.713323  579801 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I1017 20:26:24.716050  579801 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1017 20:26:24.725974  579801 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1017 20:26:24.758531  579801 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1017 20:26:24.771038  579801 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1017 20:26:24.782330  579801 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1017 20:26:24.790954  579801 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1017 20:26:24.827788  579801 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1017 20:26:25.513739  579801 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I1017 20:26:25.565865  579801 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I1017 20:26:25.568634  579801 kubeadm.go:322] 
I1017 20:26:25.568862  579801 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I1017 20:26:25.568873  579801 kubeadm.go:322] 
I1017 20:26:25.569052  579801 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I1017 20:26:25.569059  579801 kubeadm.go:322] 
I1017 20:26:25.569125  579801 kubeadm.go:322]   mkdir -p $HOME/.kube
I1017 20:26:25.569319  579801 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1017 20:26:25.569443  579801 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1017 20:26:25.569449  579801 kubeadm.go:322] 
I1017 20:26:25.569587  579801 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I1017 20:26:25.569594  579801 kubeadm.go:322] 
I1017 20:26:25.569711  579801 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1017 20:26:25.569720  579801 kubeadm.go:322] 
I1017 20:26:25.569867  579801 kubeadm.go:322] You should now deploy a pod network to the cluster.
I1017 20:26:25.570058  579801 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1017 20:26:25.570218  579801 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1017 20:26:25.570225  579801 kubeadm.go:322] 
I1017 20:26:25.570516  579801 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I1017 20:26:25.570699  579801 kubeadm.go:322] and service account keys on each node and then running the following as root:
I1017 20:26:25.570707  579801 kubeadm.go:322] 
I1017 20:26:25.570875  579801 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token vn2rep.6qrku7ag74yp5z7c \
I1017 20:26:25.571089  579801 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:53c31f1883407950d3e1480925cfb3d2c356a51af22c5dd511a3dc374a520a03 \
I1017 20:26:25.571126  579801 kubeadm.go:322] 	--control-plane 
I1017 20:26:25.571131  579801 kubeadm.go:322] 
I1017 20:26:25.571406  579801 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I1017 20:26:25.571421  579801 kubeadm.go:322] 
I1017 20:26:25.571699  579801 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token vn2rep.6qrku7ag74yp5z7c \
I1017 20:26:25.572178  579801 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:53c31f1883407950d3e1480925cfb3d2c356a51af22c5dd511a3dc374a520a03 
I1017 20:26:25.575516  579801 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1017 20:26:25.578572  579801 cni.go:84] Creating CNI manager for ""
I1017 20:26:25.579745  579801 cni.go:158] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1017 20:26:25.583115  579801 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1017 20:26:25.587754  579801 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1017 20:26:25.615537  579801 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1017 20:26:25.672984  579801 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1017 20:26:25.673331  579801 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.4/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1017 20:26:25.674210  579801 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.4/kubectl label nodes minikube.k8s.io/version=v1.31.2 minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_10_17T20_26_25_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1017 20:26:26.462655  579801 kubeadm.go:1081] duration metric: took 789.528425ms to wait for elevateKubeSystemPrivileges.
I1017 20:26:26.462770  579801 ops.go:34] apiserver oom_adj: -16
I1017 20:26:26.489770  579801 host.go:66] Checking if "minikube" exists ...
I1017 20:26:26.499989  579801 main.go:141] libmachine: Using SSH client type: external
I1017 20:26:26.500035  579801 main.go:141] libmachine: Using SSH private key: /home/akshay/.minikube/machines/minikube/id_rsa (-rw-------)
I1017 20:26:26.500085  579801 main.go:141] libmachine: &{[-F /dev/null -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none -o LogLevel=quiet -o PasswordAuthentication=no -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null docker@localhost -o IdentitiesOnly=yes -i /home/akshay/.minikube/machines/minikube/id_rsa -p 41589] /usr/bin/ssh <nil>}
I1017 20:26:26.500123  579801 main.go:141] libmachine: /usr/bin/ssh -F /dev/null -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none -o LogLevel=quiet -o PasswordAuthentication=no -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null docker@localhost -o IdentitiesOnly=yes -i /home/akshay/.minikube/machines/minikube/id_rsa -p 41589 -f -NTL 36739:localhost:8443
I1017 20:26:26.679866  579801 kubeadm.go:406] StartCluster complete in 28.214372046s
I1017 20:26:26.680518  579801 settings.go:142] acquiring lock: {Name:mk7013437d0342913e41d2072bdca6347081f4d7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:26:26.682095  579801 settings.go:150] Updating kubeconfig:  /home/akshay/.kube/config
I1017 20:26:26.688755  579801 lock.go:35] WriteFile acquiring /home/akshay/.kube/config: {Name:mk2ecfed87493c822fb99b92e6d016e49bd9718b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1017 20:26:26.689582  579801 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1017 20:26:26.689840  579801 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1017 20:26:26.690835  579801 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1017 20:26:26.690915  579801 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1017 20:26:26.690988  579801 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1017 20:26:26.690998  579801 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1017 20:26:26.691757  579801 config.go:182] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1017 20:26:26.691763  579801 host.go:66] Checking if "minikube" exists ...
I1017 20:26:26.697462  579801 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1017 20:26:26.698428  579801 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1017 20:26:26.698442  579801 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1017 20:26:26.698467  579801 sshutil.go:53] new ssh client: &{IP:localhost Port:41589 SSHKeyPath:/home/akshay/.minikube/machines/minikube/id_rsa Username:docker}
I1017 20:26:26.753971  579801 addons.go:231] Setting addon default-storageclass=true in "minikube"
I1017 20:26:26.754030  579801 host.go:66] Checking if "minikube" exists ...
I1017 20:26:26.755740  579801 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1017 20:26:26.755755  579801 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1017 20:26:26.755772  579801 sshutil.go:53] new ssh client: &{IP:localhost Port:41589 SSHKeyPath:/home/akshay/.minikube/machines/minikube/id_rsa Username:docker}
I1017 20:26:26.790610  579801 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1017 20:26:26.790705  579801 start.go:223] Will wait 6m0s for node &{Name: IP:10.0.2.15 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1017 20:26:26.791436  579801 out.go:177] üîé  Verifying Kubernetes components...
I1017 20:26:26.792664  579801 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1017 20:26:26.901471  579801 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1017 20:26:26.936544  579801 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1017 20:26:27.317449  579801 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           10.0.2.2 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1017 20:26:27.319388  579801 api_server.go:52] waiting for apiserver process to appear ...
I1017 20:26:27.319473  579801 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1017 20:26:29.046837  579801 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.145305218s)
I1017 20:26:29.381154  579801 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.444568298s)
I1017 20:26:29.381223  579801 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           10.0.2.2 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (2.063747371s)
I1017 20:26:29.381308  579801 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.061806089s)
I1017 20:26:29.381339  579801 api_server.go:72] duration metric: took 2.590604091s to wait for apiserver process to appear ...
I1017 20:26:29.381349  579801 api_server.go:88] waiting for apiserver healthz status ...
I1017 20:26:29.381249  579801 start.go:901] {"host.minikube.internal": 10.0.2.2} host record injected into CoreDNS's ConfigMap
I1017 20:26:29.381568  579801 api_server.go:253] Checking apiserver healthz at https://localhost:36739/healthz ...
I1017 20:26:29.382437  579801 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I1017 20:26:29.383732  579801 addons.go:502] enable addons completed in 2.69389354s: enabled=[default-storageclass storage-provisioner]
I1017 20:26:29.398711  579801 api_server.go:279] https://localhost:36739/healthz returned 200:
ok
I1017 20:26:29.404621  579801 api_server.go:141] control plane version: v1.27.4
I1017 20:26:29.404647  579801 api_server.go:131] duration metric: took 23.290999ms to wait for apiserver health ...
I1017 20:26:29.404658  579801 system_pods.go:43] waiting for kube-system pods to appear ...
I1017 20:26:29.418021  579801 system_pods.go:59] 5 kube-system pods found
I1017 20:26:29.418058  579801 system_pods.go:61] "etcd-minikube" [5da0200e-962e-404d-8107-db3d61cd0daa] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1017 20:26:29.418072  579801 system_pods.go:61] "kube-apiserver-minikube" [711eaf30-464a-495b-a38b-c4e79fabba8b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1017 20:26:29.418083  579801 system_pods.go:61] "kube-controller-manager-minikube" [47f5f6cf-b94d-4ad0-ab9a-bc9acf0b37e3] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1017 20:26:29.418092  579801 system_pods.go:61] "kube-scheduler-minikube" [95a6183d-026c-41ac-8d5c-d8cb6f2d0e64] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1017 20:26:29.418100  579801 system_pods.go:61] "storage-provisioner" [ee129608-7fc3-4490-bbbe-6570bb467956] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..)
I1017 20:26:29.418112  579801 system_pods.go:74] duration metric: took 13.447398ms to wait for pod list to return data ...
I1017 20:26:29.418129  579801 kubeadm.go:581] duration metric: took 2.627393112s to wait for : map[apiserver:true system_pods:true] ...
I1017 20:26:29.418157  579801 node_conditions.go:102] verifying NodePressure condition ...
I1017 20:26:29.424575  579801 node_conditions.go:122] node storage ephemeral capacity is 17784760Ki
I1017 20:26:29.424886  579801 node_conditions.go:123] node cpu capacity is 2
I1017 20:26:29.424932  579801 node_conditions.go:105] duration metric: took 6.769996ms to run NodePressure ...
I1017 20:26:29.424951  579801 start.go:228] waiting for startup goroutines ...
I1017 20:26:29.424962  579801 start.go:233] waiting for cluster config update ...
I1017 20:26:29.424977  579801 start.go:242] writing updated cluster config ...
I1017 20:26:29.425664  579801 ssh_runner.go:195] Run: rm -f paused
I1017 20:26:29.581697  579801 start.go:600] kubectl: 1.28.2, cluster: 1.27.4 (minor skew: 1)
I1017 20:26:29.582846  579801 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Journal begins at Tue 2023-10-17 14:55:11 UTC, ends at Tue 2023-10-17 14:59:30 UTC. --
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.944516202Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.948229425Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.948308501Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.948364406Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.954197013Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.954414982Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.954523464Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.954595630Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.979985660Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.980317383Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.980413384Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:10 minikube dockerd[1090]: time="2023-10-17T14:56:10.980473547Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:11 minikube cri-dockerd[979]: time="2023-10-17T14:56:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7b8ff3550797d201db27bf49e8223e306e91152a92befb8e24d3af7bf5b5c445/resolv.conf as [nameserver 10.0.2.3]"
Oct 17 14:56:12 minikube dockerd[1090]: time="2023-10-17T14:56:12.212696359Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:12 minikube dockerd[1090]: time="2023-10-17T14:56:12.213702717Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:12 minikube dockerd[1090]: time="2023-10-17T14:56:12.213872307Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:12 minikube dockerd[1090]: time="2023-10-17T14:56:12.213991970Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:12 minikube cri-dockerd[979]: time="2023-10-17T14:56:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/45a059ffee0a6b0a2139dfc4c830798ab5f2c43ad77adcf673507eb6ee333450/resolv.conf as [nameserver 10.0.2.3]"
Oct 17 14:56:12 minikube dockerd[1090]: time="2023-10-17T14:56:12.850291688Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:12 minikube dockerd[1090]: time="2023-10-17T14:56:12.850560375Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:12 minikube dockerd[1090]: time="2023-10-17T14:56:12.850684178Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:12 minikube dockerd[1090]: time="2023-10-17T14:56:12.850759245Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:13 minikube cri-dockerd[979]: time="2023-10-17T14:56:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bd8d9f4dc879459cdfc69e9e29f2b3aea8008b4d4e0ac823e3f23dd90acbfd88/resolv.conf as [nameserver 10.0.2.3]"
Oct 17 14:56:13 minikube dockerd[1090]: time="2023-10-17T14:56:13.670727806Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:13 minikube dockerd[1090]: time="2023-10-17T14:56:13.671797295Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:13 minikube dockerd[1090]: time="2023-10-17T14:56:13.671964459Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:13 minikube dockerd[1090]: time="2023-10-17T14:56:13.672085167Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:13 minikube cri-dockerd[979]: time="2023-10-17T14:56:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7cadd0adaf55be75ac4965703300491124093f039176e030c890a83d77cf0762/resolv.conf as [nameserver 10.0.2.3]"
Oct 17 14:56:14 minikube dockerd[1090]: time="2023-10-17T14:56:14.184606284Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:14 minikube dockerd[1090]: time="2023-10-17T14:56:14.184937762Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:14 minikube dockerd[1090]: time="2023-10-17T14:56:14.185078857Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:14 minikube dockerd[1090]: time="2023-10-17T14:56:14.189689318Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:37 minikube dockerd[1090]: time="2023-10-17T14:56:37.870224310Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:37 minikube dockerd[1090]: time="2023-10-17T14:56:37.870441361Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:37 minikube dockerd[1090]: time="2023-10-17T14:56:37.870729705Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:37 minikube dockerd[1090]: time="2023-10-17T14:56:37.870834999Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.014253410Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.014530713Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.014901360Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.014987980Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.265795329Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.266064653Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.266151039Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.266217256Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:38 minikube cri-dockerd[979]: time="2023-10-17T14:56:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1e15841e1dc4f2362addf7221b746b68c7a3cf0d48b77a530ff46944cba4ecf/resolv.conf as [nameserver 10.0.2.3]"
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.768252712Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.768575839Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.773639408Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:38 minikube dockerd[1090]: time="2023-10-17T14:56:38.773701532Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:39 minikube cri-dockerd[979]: time="2023-10-17T14:56:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e2246d49e6817b0d5797f419a05823cc9ef9199557110cf0dbc2fd0435b5afaa/resolv.conf as [nameserver 10.0.2.3]"
Oct 17 14:56:39 minikube cri-dockerd[979]: time="2023-10-17T14:56:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/19c41e10dd23ce2c357acd75fbce91dfdb0e5e1ea097eeeab9d3d9d503ed118a/resolv.conf as [nameserver 10.0.2.3]"
Oct 17 14:56:39 minikube dockerd[1090]: time="2023-10-17T14:56:39.827333855Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:39 minikube dockerd[1090]: time="2023-10-17T14:56:39.827601502Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:39 minikube dockerd[1090]: time="2023-10-17T14:56:39.827766921Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:39 minikube dockerd[1090]: time="2023-10-17T14:56:39.827804017Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:40 minikube dockerd[1090]: time="2023-10-17T14:56:40.000041018Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 17 14:56:40 minikube dockerd[1090]: time="2023-10-17T14:56:40.000194324Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:40 minikube dockerd[1090]: time="2023-10-17T14:56:40.000255622Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 17 14:56:40 minikube dockerd[1090]: time="2023-10-17T14:56:40.000298022Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 17 14:56:46 minikube cri-dockerd[979]: time="2023-10-17T14:56:46Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID
980bb3f53667f       6e38f40d628db       2 minutes ago       Running             storage-provisioner       0                   19c41e10dd23c
23ddd517b738f       ead0a4a53df89       2 minutes ago       Running             coredns                   0                   e2246d49e6817
e167f63990ac1       6848d7eda0341       2 minutes ago       Running             kube-proxy                0                   d1e15841e1dc4
6640b9e7bcfd6       98ef2570f3cde       3 minutes ago       Running             kube-scheduler            0                   7cadd0adaf55b
c7872d3263817       86b6af7dd652c       3 minutes ago       Running             etcd                      0                   bd8d9f4dc8794
bdd8b4a6b57ed       f466468864b7a       3 minutes ago       Running             kube-controller-manager   0                   45a059ffee0a6
97f1af5cc3ef1       e7972205b6614       3 minutes ago       Running             kube-apiserver            0                   7b8ff3550797d

* 
* ==> coredns [23ddd517b738] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 4369d49e705690634e66dc4876ba448687add67b4e702a1c8bd9cbe26bf81de42209d08c6b52f2167c69004abbe79b233480d7bb5830c218d455f30e7efd3686
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:48044 - 2817 "HINFO IN 5915087517381683168.7609932321186129710. udp 57 false 512" NXDOMAIN qr,rd,ra 132 1.677552683s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_10_17T20_26_25_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 17 Oct 2023 14:56:20 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 17 Oct 2023 14:59:29 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 17 Oct 2023 14:56:46 +0000   Tue, 17 Oct 2023 14:56:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 17 Oct 2023 14:56:46 +0000   Tue, 17 Oct 2023 14:56:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 17 Oct 2023 14:56:46 +0000   Tue, 17 Oct 2023 14:56:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 17 Oct 2023 14:56:46 +0000   Tue, 17 Oct 2023 14:56:28 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.0.2.15
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784760Ki
  hugepages-2Mi:      0
  memory:             2165924Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784760Ki
  hugepages-2Mi:      0
  memory:             2165924Ki
  pods:               110
System Info:
  Machine ID:                 c3a3cfefaff94c81a95dfcd199028b62
  System UUID:                c3a3cfefaff94c81a95dfcd199028b62
  Boot ID:                    2e6e4d2b-11eb-4153-ac12-c999d871f1e2
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-5d78c9869d-cmt6f            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (3%!)(MISSING)        170Mi (8%!)(MISSING)     2m53s
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (4%!)(MISSING)       0 (0%!)(MISSING)         3m4s
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m7s
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m7s
  kube-system                 kube-proxy-57c8m                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m53s
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m8s
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m1s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (8%!)(MISSING)  170Mi (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 2m50s                  kube-proxy       
  Normal  NodeAllocatableEnforced  3m21s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  3m20s (x8 over 3m21s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m20s (x8 over 3m21s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m20s (x7 over 3m21s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 3m5s                   kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  3m5s                   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m5s                   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m5s                   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  3m4s                   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                3m2s                   kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           2m54s                  node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Oct17 14:54] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000001] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.205746] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[Oct17 14:55] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +6.173426] systemd-fstab-generator[101]: Ignoring "noauto" for root device
[  +0.252476] systemd[1]: systemd-journald.service: unit configures an IP firewall, but the local system does not support BPF/cgroup firewalling.
[  +0.000008] systemd[1]: (This warning is only shown for the first unit using IP firewalling.)
[  +6.299768] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000024] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000005] NFSD: Unable to initialize client recovery tracking! (-2)
[  +9.570101] systemd-fstab-generator[518]: Ignoring "noauto" for root device
[  +0.229101] systemd-fstab-generator[529]: Ignoring "noauto" for root device
[  +2.036021] systemd-fstab-generator[702]: Ignoring "noauto" for root device
[  +0.632700] systemd-fstab-generator[743]: Ignoring "noauto" for root device
[  +0.230192] systemd-fstab-generator[754]: Ignoring "noauto" for root device
[  +0.263491] systemd-fstab-generator[767]: Ignoring "noauto" for root device
[  +1.818357] systemd-fstab-generator[924]: Ignoring "noauto" for root device
[  +0.247786] systemd-fstab-generator[935]: Ignoring "noauto" for root device
[  +0.235283] systemd-fstab-generator[946]: Ignoring "noauto" for root device
[  +0.241624] systemd-fstab-generator[957]: Ignoring "noauto" for root device
[  +0.265155] systemd-fstab-generator[971]: Ignoring "noauto" for root device
[ +21.316869] systemd-fstab-generator[1075]: Ignoring "noauto" for root device
[  +1.701003] kauditd_printk_skb: 53 callbacks suppressed
[Oct17 14:56] systemd-fstab-generator[1381]: Ignoring "noauto" for root device
[  +1.003540] kauditd_printk_skb: 29 callbacks suppressed
[  +0.758649] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[ +14.465944] systemd-fstab-generator[2336]: Ignoring "noauto" for root device

* 
* ==> etcd [c7872d326381] <==
* {"level":"info","ts":"2023-10-17T14:56:16.437Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://10.0.2.15:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://10.0.2.15:2380","--initial-cluster=minikube=https://10.0.2.15:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://10.0.2.15:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-10-17T14:56:16.439Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://10.0.2.15:2380"]}
{"level":"info","ts":"2023-10-17T14:56:16.440Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-10-17T14:56:16.440Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"]}
{"level":"info","ts":"2023-10-17T14:56:16.441Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://10.0.2.15:2380"],"listen-peer-urls":["https://10.0.2.15:2380"],"advertise-client-urls":["https://10.0.2.15:2379"],"listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://10.0.2.15:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-10-17T14:56:16.463Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"20.958587ms"}
{"level":"info","ts":"2023-10-17T14:56:16.495Z","caller":"etcdserver/raft.go:494","msg":"starting local member","local-member-id":"f074a195de705325","cluster-id":"ef296cf39f5d9d66"}
{"level":"info","ts":"2023-10-17T14:56:16.496Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 switched to configuration voters=()"}
{"level":"info","ts":"2023-10-17T14:56:16.496Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became follower at term 0"}
{"level":"info","ts":"2023-10-17T14:56:16.496Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft f074a195de705325 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2023-10-17T14:56:16.496Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became follower at term 1"}
{"level":"info","ts":"2023-10-17T14:56:16.496Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 switched to configuration voters=(17326651331455243045)"}
{"level":"warn","ts":"2023-10-17T14:56:16.506Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-10-17T14:56:16.517Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2023-10-17T14:56:16.525Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-10-17T14:56:16.533Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"f074a195de705325","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2023-10-17T14:56:16.537Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"f074a195de705325","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-10-17T14:56:16.538Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-10-17T14:56:16.538Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-10-17T14:56:16.538Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-10-17T14:56:16.549Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 switched to configuration voters=(17326651331455243045)"}
{"level":"info","ts":"2023-10-17T14:56:16.550Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"ef296cf39f5d9d66","local-member-id":"f074a195de705325","added-peer-id":"f074a195de705325","added-peer-peer-urls":["https://10.0.2.15:2380"]}
{"level":"info","ts":"2023-10-17T14:56:16.562Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-10-17T14:56:16.563Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"10.0.2.15:2380"}
{"level":"info","ts":"2023-10-17T14:56:16.563Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"10.0.2.15:2380"}
{"level":"info","ts":"2023-10-17T14:56:16.564Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"f074a195de705325","initial-advertise-peer-urls":["https://10.0.2.15:2380"],"listen-peer-urls":["https://10.0.2.15:2380"],"advertise-client-urls":["https://10.0.2.15:2379"],"listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-10-17T14:56:16.564Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-10-17T14:56:17.398Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 is starting a new election at term 1"}
{"level":"info","ts":"2023-10-17T14:56:17.398Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became pre-candidate at term 1"}
{"level":"info","ts":"2023-10-17T14:56:17.398Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 received MsgPreVoteResp from f074a195de705325 at term 1"}
{"level":"info","ts":"2023-10-17T14:56:17.398Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became candidate at term 2"}
{"level":"info","ts":"2023-10-17T14:56:17.398Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 received MsgVoteResp from f074a195de705325 at term 2"}
{"level":"info","ts":"2023-10-17T14:56:17.398Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became leader at term 2"}
{"level":"info","ts":"2023-10-17T14:56:17.398Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: f074a195de705325 elected leader f074a195de705325 at term 2"}
{"level":"info","ts":"2023-10-17T14:56:17.424Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"f074a195de705325","local-member-attributes":"{Name:minikube ClientURLs:[https://10.0.2.15:2379]}","request-path":"/0/members/f074a195de705325/attributes","cluster-id":"ef296cf39f5d9d66","publish-timeout":"7s"}
{"level":"info","ts":"2023-10-17T14:56:17.424Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-10-17T14:56:17.426Z","caller":"etcdserver/server.go:2571","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2023-10-17T14:56:17.428Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-10-17T14:56:17.428Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-10-17T14:56:17.429Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-10-17T14:56:17.433Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"10.0.2.15:2379"}
{"level":"info","ts":"2023-10-17T14:56:17.440Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-10-17T14:56:17.466Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"ef296cf39f5d9d66","local-member-id":"f074a195de705325","cluster-version":"3.5"}
{"level":"info","ts":"2023-10-17T14:56:17.466Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-10-17T14:56:17.466Z","caller":"etcdserver/server.go:2595","msg":"cluster version is updated","cluster-version":"3.5"}

* 
* ==> kernel <==
*  14:59:30 up 4 min,  0 users,  load average: 0.30, 0.45, 0.22
Linux minikube 5.10.57 #1 SMP Sat Jul 15 01:42:36 UTC 2023 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-apiserver [97f1af5cc3ef] <==
* I1017 14:56:20.406411       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1017 14:56:20.407368       1 controller.go:121] Starting legacy_token_tracking_controller
I1017 14:56:20.407417       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I1017 14:56:20.409415       1 system_namespaces_controller.go:67] Starting system namespaces controller
I1017 14:56:20.409604       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1017 14:56:20.409664       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1017 14:56:20.409808       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I1017 14:56:20.410083       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I1017 14:56:20.410582       1 available_controller.go:423] Starting AvailableConditionController
I1017 14:56:20.410628       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1017 14:56:20.410865       1 controller.go:83] Starting OpenAPI AggregationController
I1017 14:56:20.411578       1 controller.go:85] Starting OpenAPI controller
I1017 14:56:20.411819       1 controller.go:85] Starting OpenAPI V3 controller
I1017 14:56:20.411953       1 naming_controller.go:291] Starting NamingConditionController
I1017 14:56:20.412102       1 establishing_controller.go:76] Starting EstablishingController
I1017 14:56:20.412266       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1017 14:56:20.412379       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1017 14:56:20.412524       1 crd_finalizer.go:266] Starting CRDFinalizer
I1017 14:56:20.413268       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1017 14:56:20.413449       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1017 14:56:20.413516       1 aggregator.go:150] waiting for initial CRD sync...
I1017 14:56:20.413557       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I1017 14:56:20.413984       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1017 14:56:20.414028       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I1017 14:56:20.414263       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1017 14:56:20.414458       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1017 14:56:20.484906       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1017 14:56:20.485198       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I1017 14:56:20.485422       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1017 14:56:20.485729       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1017 14:56:20.680915       1 shared_informer.go:318] Caches are synced for configmaps
I1017 14:56:20.682333       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1017 14:56:20.682407       1 apf_controller.go:366] Running API Priority and Fairness config worker
I1017 14:56:20.682652       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I1017 14:56:20.682971       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1017 14:56:20.687489       1 shared_informer.go:318] Caches are synced for crd-autoregister
I1017 14:56:20.689132       1 aggregator.go:152] initial CRD sync complete...
I1017 14:56:20.689276       1 autoregister_controller.go:141] Starting autoregister controller
I1017 14:56:20.689339       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1017 14:56:20.689399       1 cache.go:39] Caches are synced for autoregister controller
I1017 14:56:20.696316       1 shared_informer.go:318] Caches are synced for node_authorizer
I1017 14:56:20.708292       1 controller.go:624] quota admission added evaluator for: namespaces
I1017 14:56:20.715982       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I1017 14:56:20.785216       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I1017 14:56:20.939728       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1017 14:56:21.424066       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1017 14:56:21.448549       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1017 14:56:21.448601       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1017 14:56:22.869477       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1017 14:56:23.046449       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1017 14:56:23.347436       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W1017 14:56:23.362570       1 lease.go:251] Resetting endpoints for master service "kubernetes" to [10.0.2.15]
I1017 14:56:23.365451       1 controller.go:624] quota admission added evaluator for: endpoints
I1017 14:56:23.375167       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1017 14:56:23.602300       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I1017 14:56:25.514505       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1017 14:56:25.544267       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I1017 14:56:25.562029       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I1017 14:56:37.284040       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1017 14:56:37.342913       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps

* 
* ==> kube-controller-manager [bdd8b4a6b57e] <==
* I1017 14:56:36.170654       1 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I1017 14:56:36.170678       1 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I1017 14:56:36.322501       1 controllermanager.go:638] "Started controller" controller="ttl-after-finished"
I1017 14:56:36.323885       1 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I1017 14:56:36.323918       1 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I1017 14:56:36.330837       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I1017 14:56:36.372725       1 shared_informer.go:318] Caches are synced for PV protection
I1017 14:56:36.387708       1 shared_informer.go:318] Caches are synced for namespace
I1017 14:56:36.395771       1 shared_informer.go:318] Caches are synced for expand
I1017 14:56:36.401447       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1017 14:56:36.409933       1 shared_informer.go:318] Caches are synced for crt configmap
I1017 14:56:36.417855       1 shared_informer.go:318] Caches are synced for stateful set
I1017 14:56:36.424425       1 shared_informer.go:318] Caches are synced for TTL after finished
I1017 14:56:36.418752       1 shared_informer.go:318] Caches are synced for HPA
I1017 14:56:36.424977       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1017 14:56:36.425870       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1017 14:56:36.429212       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1017 14:56:36.435116       1 shared_informer.go:318] Caches are synced for node
I1017 14:56:36.438799       1 range_allocator.go:174] "Sending events to api server"
I1017 14:56:36.441097       1 shared_informer.go:318] Caches are synced for GC
I1017 14:56:36.445671       1 shared_informer.go:318] Caches are synced for ReplicationController
I1017 14:56:36.449501       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1017 14:56:36.450527       1 shared_informer.go:318] Caches are synced for endpoint
I1017 14:56:36.450716       1 shared_informer.go:318] Caches are synced for service account
I1017 14:56:36.450986       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1017 14:56:36.451240       1 range_allocator.go:178] "Starting range CIDR allocator"
I1017 14:56:36.451268       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1017 14:56:36.453415       1 shared_informer.go:318] Caches are synced for TTL
I1017 14:56:36.461258       1 shared_informer.go:318] Caches are synced for PVC protection
I1017 14:56:36.463928       1 shared_informer.go:318] Caches are synced for cidrallocator
I1017 14:56:36.466294       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1017 14:56:36.468478       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1017 14:56:36.470473       1 shared_informer.go:318] Caches are synced for job
I1017 14:56:36.472765       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1017 14:56:36.479019       1 shared_informer.go:318] Caches are synced for ephemeral
I1017 14:56:36.482911       1 shared_informer.go:318] Caches are synced for cronjob
I1017 14:56:36.485701       1 shared_informer.go:318] Caches are synced for persistent volume
I1017 14:56:36.487884       1 shared_informer.go:318] Caches are synced for attach detach
I1017 14:56:36.489979       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1017 14:56:36.489878       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=[10.244.0.0/24]
I1017 14:56:36.502104       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1017 14:56:36.503709       1 shared_informer.go:318] Caches are synced for deployment
I1017 14:56:36.513278       1 shared_informer.go:318] Caches are synced for disruption
I1017 14:56:36.557157       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1017 14:56:36.568976       1 shared_informer.go:318] Caches are synced for taint
I1017 14:56:36.569454       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1017 14:56:36.569764       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1017 14:56:36.569951       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1017 14:56:36.570304       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1017 14:56:36.570657       1 taint_manager.go:211] "Sending events to api server"
I1017 14:56:36.570801       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1017 14:56:36.620789       1 shared_informer.go:318] Caches are synced for daemon sets
I1017 14:56:36.631905       1 shared_informer.go:318] Caches are synced for resource quota
I1017 14:56:36.632401       1 shared_informer.go:318] Caches are synced for resource quota
I1017 14:56:36.959026       1 shared_informer.go:318] Caches are synced for garbage collector
I1017 14:56:36.959091       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1017 14:56:36.969450       1 shared_informer.go:318] Caches are synced for garbage collector
I1017 14:56:37.290278       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5d78c9869d to 1"
I1017 14:56:37.363676       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-57c8m"
I1017 14:56:37.501739       1 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5d78c9869d-cmt6f"

* 
* ==> kube-proxy [e167f63990ac] <==
* I1017 14:56:39.278808       1 node.go:141] Successfully retrieved node IP: 10.0.2.15
I1017 14:56:39.279274       1 server_others.go:110] "Detected node IP" address="10.0.2.15"
I1017 14:56:39.279457       1 server_others.go:554] "Using iptables proxy"
I1017 14:56:39.437503       1 server_others.go:178] "kube-proxy running in single-stack mode: secondary ipFamily is not supported" ipFamily=IPv6
I1017 14:56:39.437549       1 server_others.go:192] "Using iptables Proxier"
I1017 14:56:39.437647       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1017 14:56:39.438494       1 server.go:658] "Version info" version="v1.27.4"
I1017 14:56:39.438516       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1017 14:56:39.440285       1 config.go:188] "Starting service config controller"
I1017 14:56:39.440425       1 shared_informer.go:311] Waiting for caches to sync for service config
I1017 14:56:39.440561       1 config.go:97] "Starting endpoint slice config controller"
I1017 14:56:39.440654       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1017 14:56:39.443866       1 config.go:315] "Starting node config controller"
I1017 14:56:39.444159       1 shared_informer.go:311] Waiting for caches to sync for node config
I1017 14:56:39.541173       1 shared_informer.go:318] Caches are synced for service config
I1017 14:56:39.541485       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1017 14:56:39.544229       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [6640b9e7bcfd] <==
* E1017 14:56:20.721664       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1017 14:56:20.729143       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:20.729220       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1017 14:56:20.729464       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1017 14:56:20.729501       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1017 14:56:20.729771       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:20.729824       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1017 14:56:20.730033       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:20.730068       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1017 14:56:20.730483       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1017 14:56:20.732817       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1017 14:56:20.730579       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1017 14:56:20.734320       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1017 14:56:20.745438       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1017 14:56:20.745893       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1017 14:56:20.746439       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1017 14:56:20.746722       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1017 14:56:20.747185       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1017 14:56:20.753190       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1017 14:56:20.747281       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1017 14:56:20.753810       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1017 14:56:20.747369       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1017 14:56:20.747471       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1017 14:56:20.747567       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1017 14:56:20.747703       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:20.756538       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:20.756843       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1017 14:56:20.757030       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1017 14:56:20.757213       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1017 14:56:21.638416       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:21.638466       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1017 14:56:21.742435       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:21.742493       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1017 14:56:21.759977       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1017 14:56:21.760041       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1017 14:56:21.798063       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1017 14:56:21.798224       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1017 14:56:21.827815       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1017 14:56:21.827872       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1017 14:56:21.858171       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1017 14:56:21.858233       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1017 14:56:21.869064       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1017 14:56:21.869141       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1017 14:56:22.048396       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:22.048690       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1017 14:56:22.173339       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1017 14:56:22.173908       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1017 14:56:22.212507       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1017 14:56:22.212719       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1017 14:56:22.237216       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1017 14:56:22.237323       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1017 14:56:22.285261       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1017 14:56:22.285346       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1017 14:56:22.287402       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1017 14:56:22.287439       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1017 14:56:22.295250       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1017 14:56:22.295359       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1017 14:56:22.323113       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1017 14:56:22.323549       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
I1017 14:56:24.910339       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Journal begins at Tue 2023-10-17 14:55:11 UTC, ends at Tue 2023-10-17 14:59:30 UTC. --
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.228107    2385 state_mem.go:35] "Initializing new in-memory state store"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.228462    2385 state_mem.go:75] "Updated machine memory state"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.231855    2385 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.235398    2385 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.612994    2385 topology_manager.go:212] "Topology Admit Handler"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.613228    2385 topology_manager.go:212] "Topology Admit Handler"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.613343    2385 topology_manager.go:212] "Topology Admit Handler"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.613460    2385 topology_manager.go:212] "Topology Admit Handler"
Oct 17 14:56:26 minikube kubelet[2385]: E1017 14:56:26.635456    2385 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: E1017 14:56:26.638127    2385 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: E1017 14:56:26.650751    2385 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.711872    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/d3bb814607b61262cc219c8020a27fc0-etcd-certs\") pod \"etcd-minikube\" (UID: \"d3bb814607b61262cc219c8020a27fc0\") " pod="kube-system/etcd-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.712437    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/77b94530d08aa7cb7fdaa3ac7fcc7cbb-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"77b94530d08aa7cb7fdaa3ac7fcc7cbb\") " pod="kube-system/kube-apiserver-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.712758    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/77b94530d08aa7cb7fdaa3ac7fcc7cbb-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"77b94530d08aa7cb7fdaa3ac7fcc7cbb\") " pod="kube-system/kube-apiserver-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.713546    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ee9c695ad9ff256c3f892f7e252e24d8-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ee9c695ad9ff256c3f892f7e252e24d8\") " pod="kube-system/kube-controller-manager-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.714493    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ee9c695ad9ff256c3f892f7e252e24d8-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ee9c695ad9ff256c3f892f7e252e24d8\") " pod="kube-system/kube-controller-manager-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.714676    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/eb675835e10503c79265cf0e2983f93c-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"eb675835e10503c79265cf0e2983f93c\") " pod="kube-system/kube-scheduler-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.714853    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/d3bb814607b61262cc219c8020a27fc0-etcd-data\") pod \"etcd-minikube\" (UID: \"d3bb814607b61262cc219c8020a27fc0\") " pod="kube-system/etcd-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.715080    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/77b94530d08aa7cb7fdaa3ac7fcc7cbb-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"77b94530d08aa7cb7fdaa3ac7fcc7cbb\") " pod="kube-system/kube-apiserver-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.715254    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ee9c695ad9ff256c3f892f7e252e24d8-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ee9c695ad9ff256c3f892f7e252e24d8\") " pod="kube-system/kube-controller-manager-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.715346    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ee9c695ad9ff256c3f892f7e252e24d8-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ee9c695ad9ff256c3f892f7e252e24d8\") " pod="kube-system/kube-controller-manager-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.715531    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ee9c695ad9ff256c3f892f7e252e24d8-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ee9c695ad9ff256c3f892f7e252e24d8\") " pod="kube-system/kube-controller-manager-minikube"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.808218    2385 apiserver.go:52] "Watching apiserver"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.874005    2385 desired_state_of_world_populator.go:153] "Finished populating initial desired state of world"
Oct 17 14:56:26 minikube kubelet[2385]: I1017 14:56:26.918998    2385 reconciler.go:41] "Reconciler: start to sync state"
Oct 17 14:56:27 minikube kubelet[2385]: I1017 14:56:27.037781    2385 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.037658198 podCreationTimestamp="2023-10-17 14:56:26 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-10-17 14:56:27.015534184 +0000 UTC m=+1.563853287" watchObservedRunningTime="2023-10-17 14:56:27.037658198 +0000 UTC m=+1.585977312"
Oct 17 14:56:28 minikube kubelet[2385]: I1017 14:56:28.071595    2385 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
Oct 17 14:56:36 minikube kubelet[2385]: I1017 14:56:36.601460    2385 topology_manager.go:212] "Topology Admit Handler"
Oct 17 14:56:36 minikube kubelet[2385]: I1017 14:56:36.717028    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5wlv4\" (UniqueName: \"kubernetes.io/projected/ee129608-7fc3-4490-bbbe-6570bb467956-kube-api-access-5wlv4\") pod \"storage-provisioner\" (UID: \"ee129608-7fc3-4490-bbbe-6570bb467956\") " pod="kube-system/storage-provisioner"
Oct 17 14:56:36 minikube kubelet[2385]: I1017 14:56:36.717172    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/ee129608-7fc3-4490-bbbe-6570bb467956-tmp\") pod \"storage-provisioner\" (UID: \"ee129608-7fc3-4490-bbbe-6570bb467956\") " pod="kube-system/storage-provisioner"
Oct 17 14:56:36 minikube kubelet[2385]: E1017 14:56:36.837640    2385 projected.go:292] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Oct 17 14:56:36 minikube kubelet[2385]: E1017 14:56:36.837918    2385 projected.go:198] Error preparing data for projected volume kube-api-access-5wlv4 for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Oct 17 14:56:36 minikube kubelet[2385]: E1017 14:56:36.838551    2385 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/ee129608-7fc3-4490-bbbe-6570bb467956-kube-api-access-5wlv4 podName:ee129608-7fc3-4490-bbbe-6570bb467956 nodeName:}" failed. No retries permitted until 2023-10-17 14:56:37.338156369 +0000 UTC m=+11.886475468 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-5wlv4" (UniqueName: "kubernetes.io/projected/ee129608-7fc3-4490-bbbe-6570bb467956-kube-api-access-5wlv4") pod "storage-provisioner" (UID: "ee129608-7fc3-4490-bbbe-6570bb467956") : configmap "kube-root-ca.crt" not found
Oct 17 14:56:37 minikube kubelet[2385]: I1017 14:56:37.385265    2385 topology_manager.go:212] "Topology Admit Handler"
Oct 17 14:56:37 minikube kubelet[2385]: I1017 14:56:37.521859    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/992ba1cb-ed03-44de-933e-91db059e9b70-lib-modules\") pod \"kube-proxy-57c8m\" (UID: \"992ba1cb-ed03-44de-933e-91db059e9b70\") " pod="kube-system/kube-proxy-57c8m"
Oct 17 14:56:37 minikube kubelet[2385]: I1017 14:56:37.522454    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cqx7s\" (UniqueName: \"kubernetes.io/projected/992ba1cb-ed03-44de-933e-91db059e9b70-kube-api-access-cqx7s\") pod \"kube-proxy-57c8m\" (UID: \"992ba1cb-ed03-44de-933e-91db059e9b70\") " pod="kube-system/kube-proxy-57c8m"
Oct 17 14:56:37 minikube kubelet[2385]: I1017 14:56:37.522758    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/992ba1cb-ed03-44de-933e-91db059e9b70-xtables-lock\") pod \"kube-proxy-57c8m\" (UID: \"992ba1cb-ed03-44de-933e-91db059e9b70\") " pod="kube-system/kube-proxy-57c8m"
Oct 17 14:56:37 minikube kubelet[2385]: I1017 14:56:37.523049    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/992ba1cb-ed03-44de-933e-91db059e9b70-kube-proxy\") pod \"kube-proxy-57c8m\" (UID: \"992ba1cb-ed03-44de-933e-91db059e9b70\") " pod="kube-system/kube-proxy-57c8m"
Oct 17 14:56:37 minikube kubelet[2385]: I1017 14:56:37.528943    2385 topology_manager.go:212] "Topology Admit Handler"
Oct 17 14:56:37 minikube kubelet[2385]: I1017 14:56:37.629073    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/88a116f5-0b02-421a-bfb3-1422ee921d3e-config-volume\") pod \"coredns-5d78c9869d-cmt6f\" (UID: \"88a116f5-0b02-421a-bfb3-1422ee921d3e\") " pod="kube-system/coredns-5d78c9869d-cmt6f"
Oct 17 14:56:37 minikube kubelet[2385]: I1017 14:56:37.631763    2385 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zcnxq\" (UniqueName: \"kubernetes.io/projected/88a116f5-0b02-421a-bfb3-1422ee921d3e-kube-api-access-zcnxq\") pod \"coredns-5d78c9869d-cmt6f\" (UID: \"88a116f5-0b02-421a-bfb3-1422ee921d3e\") " pod="kube-system/coredns-5d78c9869d-cmt6f"
Oct 17 14:56:38 minikube kubelet[2385]: I1017 14:56:38.574234    2385 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d1e15841e1dc4f2362addf7221b746b68c7a3cf0d48b77a530ff46944cba4ecf"
Oct 17 14:56:39 minikube kubelet[2385]: I1017 14:56:39.422137    2385 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e2246d49e6817b0d5797f419a05823cc9ef9199557110cf0dbc2fd0435b5afaa"
Oct 17 14:56:39 minikube kubelet[2385]: I1017 14:56:39.795027    2385 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="19c41e10dd23ce2c357acd75fbce91dfdb0e5e1ea097eeeab9d3d9d503ed118a"
Oct 17 14:56:40 minikube kubelet[2385]: I1017 14:56:40.890437    2385 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-57c8m" podStartSLOduration=3.890367307 podCreationTimestamp="2023-10-17 14:56:37 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-10-17 14:56:40.857719278 +0000 UTC m=+15.406038383" watchObservedRunningTime="2023-10-17 14:56:40.890367307 +0000 UTC m=+15.438686417"
Oct 17 14:56:40 minikube kubelet[2385]: I1017 14:56:40.916154    2385 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-5d78c9869d-cmt6f" podStartSLOduration=3.91609075 podCreationTimestamp="2023-10-17 14:56:37 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-10-17 14:56:40.890268855 +0000 UTC m=+15.438587962" watchObservedRunningTime="2023-10-17 14:56:40.91609075 +0000 UTC m=+15.464409861"
Oct 17 14:56:46 minikube kubelet[2385]: I1017 14:56:46.497566    2385 kuberuntime_manager.go:1460] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 17 14:56:46 minikube kubelet[2385]: I1017 14:56:46.498810    2385 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 17 14:57:25 minikube kubelet[2385]: E1017 14:57:25.981043    2385 iptables.go:575] "Could not set up iptables canary" err=<
Oct 17 14:57:25 minikube kubelet[2385]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 17 14:57:25 minikube kubelet[2385]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 17 14:57:25 minikube kubelet[2385]:  > table=nat chain=KUBE-KUBELET-CANARY
Oct 17 14:58:25 minikube kubelet[2385]: E1017 14:58:25.980769    2385 iptables.go:575] "Could not set up iptables canary" err=<
Oct 17 14:58:25 minikube kubelet[2385]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 17 14:58:25 minikube kubelet[2385]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 17 14:58:25 minikube kubelet[2385]:  > table=nat chain=KUBE-KUBELET-CANARY
Oct 17 14:59:25 minikube kubelet[2385]: E1017 14:59:25.981954    2385 iptables.go:575] "Could not set up iptables canary" err=<
Oct 17 14:59:25 minikube kubelet[2385]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 17 14:59:25 minikube kubelet[2385]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 17 14:59:25 minikube kubelet[2385]:  > table=nat chain=KUBE-KUBELET-CANARY

* 
* ==> storage-provisioner [980bb3f53667] <==
* I1017 14:56:40.163982       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1017 14:56:40.182033       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1017 14:56:40.182551       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1017 14:56:40.199778       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1017 14:56:40.201288       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_cae0b9e2-88ca-4ba3-977c-397de2c062c8!
I1017 14:56:40.204979       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"beffd6c1-06c9-480c-95b6-ae2ad47b9f90", APIVersion:"v1", ResourceVersion:"402", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_cae0b9e2-88ca-4ba3-977c-397de2c062c8 became leader
I1017 14:56:40.301797       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_cae0b9e2-88ca-4ba3-977c-397de2c062c8!

